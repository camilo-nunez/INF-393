{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar7LdIdqSuXC"
   },
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF-393 Aprendizaje Automático II-2020 </h1>\n",
    "    <h3> Taller 1 </h3>\n",
    "</center>\n",
    "\n",
    "Nombres: Nicolás Rosas Gómez  & Camilo Núñez Fernández\n",
    "\n",
    "Roles: 201573608-1 &201573573-5\n",
    "\n",
    "Correos:\n",
    "nicolas.rosasg@sansano.usm.cl & camilo.nunezf@sansano.usm.cl\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "\n",
    "* Clasificadores Discriminativos Clásicos\n",
    "* Clasificadores Generativos Clásicos\n",
    "* Regresión Lineal y Regularización \n",
    "* Selección de Características (parcial, seguiremos en T2)\n",
    "* Reducción de Dimensionalidad\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta de Investigación\n",
    "\n",
    "## Pregunta seleccionada:\n",
    "**Reducción de Dimensionalidad**: Es igual de efectivo aplicar PCA sobre variables numéricas continuas que aplicarlo sobre variables categóricas codificadas numéricamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "try:\n",
    "    import prince\n",
    "except ImportError:\n",
    "    os.system('pip install prince')\n",
    "    import prince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo\n",
    "\n",
    "La estrategia a desarrollar para afirmar o refutar la hipotesis definida anteriormente, constara de calcular en tres dataset distintos, la descomposicion PCA sobre sus variables continuas, categoricas y enteras de manera independiente. A continuacion se obtendrá el porcentaje de varianza y el `cumulative sum` de estos valores. Además se entrenará un modeló de regresion lineal para todos los casos, a partir del cual se comentará el MSE de los valores predichos en cada caso.\n",
    "\n",
    "Se consideraron dos dataset sintéticos y dos reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abalone dataset (Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con PCA y sin columna categorica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos cargando el dataset desde el archivo original:Comenzamos cargando el dataset desde el archivo original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data = pd.read_csv(\"abalone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiaremos el dataset, y definiremos el conjunto $Y$ como la varaible `Weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_data = abalone_data.select_dtypes(\"O\")\n",
    "X_abalone = abalone_data.loc[:, abalone_data.columns != 'Weight']\n",
    "X_abalone = X_abalone.loc[:, X_abalone.columns != 'Sex']\n",
    "y_abalone = abalone_data[\"Weight\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación asignamos el modelo de regresión para probar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos PCA y reducimos la dimencionalidad a 6 feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.2 ms, sys: 153 ms, total: 200 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_abalone, y_abalone, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(X_train, y_test)\n",
    "\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos los valores procentuales de la varianza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03807187e+01 7.31139961e-02 2.78958153e-03 2.01560681e-03\n",
      " 9.78197249e-04 5.72483154e-04]\n",
      "[9.92388314e-01 6.98963893e-03 2.66681739e-04 1.92690381e-04\n",
      " 9.35148663e-05 5.47289267e-05]\n",
      "[0.99238831 0.99937795 0.99964463 0.99983732 0.99993084 0.99998557]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego entrenamos el modelo de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.0022363715351909285\n",
      "MSE_Test: 0.0017495303027569283\n"
     ]
    }
   ],
   "source": [
    "PCA_model.fit(PCA_X_train, y_train)\n",
    "pred_train = PCA_model.predict(PCA_X_train)\n",
    "pred_test = PCA_model.predict(PCA_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con PCA y con columna categorica (One Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, connsideraremos un `One Hot Encoding` para las variables categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder().fit(categorical_data)\n",
    "categorical_bin = ohe.transform(categorical_data)\n",
    "categorical_bin = pd.DataFrame(categorical_bin.toarray(), columns=ohe.get_feature_names())\n",
    "X_abalone_cat = pd.concat([X_abalone,categorical_bin], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicamos el modelo de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model_cat = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la descomposición de PCA para este caso con `OHE`, para 6 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 377 ms, total: 500 ms\n",
      "Wall time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_abalone_cat, y_abalone, test_size=0.25, random_state=0)\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "pca.fit(X_train, y_test)\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04432385e+01 3.46225907e-01 2.73879605e-01 5.54210557e-02\n",
      " 2.76524270e-03 2.01319116e-03]\n",
      "[9.38698834e-01 3.11207923e-02 2.46178872e-02 4.98156587e-03\n",
      " 2.48556049e-04 1.80957296e-04]\n",
      "[0.93869883 0.96981963 0.99443751 0.99941908 0.99966763 0.99984859]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.002462558647550641\n",
      "MSE_Test: 0.001933513535465762\n"
     ]
    }
   ],
   "source": [
    "PCA_model_cat.fit(PCA_X_train, y_train)\n",
    "pred_train = PCA_model_cat.predict(PCA_X_train)\n",
    "pred_test = PCA_model_cat.predict(PCA_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con MCA y columna categorica (_prince package_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, aplicaremos la descomposición del método `MCA` sobre el dataset con columnas categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.012607759508113284\n",
      "MSE_Test: 0.0117558193042838\n"
     ]
    }
   ],
   "source": [
    "MCA_prince_model = LinearRegression()\n",
    "prince_mca = prince.MCA(n_components=6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_abalone_cat, y_abalone, test_size=0.25, random_state=0)\n",
    "\n",
    "prince_mca = prince_mca.fit(X_train, y_train) \n",
    "prince_mca_X_train = prince_mca.transform(X_train) \n",
    "\n",
    "prince_mca_X_test = prince_mca.transform(X_test) \n",
    "\n",
    "MCA_prince_model.fit(prince_mca_X_train, y_train)\n",
    "\n",
    "pred_train = MCA_prince_model.predict(prince_mca_X_train)\n",
    "pred_test = MCA_prince_model.predict(prince_mca_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con MCA y sin columna categorica (_prince package_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos el mismo método pero ahora sin considerar las columnas categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.014424450091002051\n",
      "MSE_Test: 0.014538565398065365\n"
     ]
    }
   ],
   "source": [
    "MCA_prince_model = LinearRegression()\n",
    "prince_mca = prince.MCA(n_components=6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_abalone, y_abalone, test_size=0.25, random_state=0)\n",
    "\n",
    "prince_mca = prince_mca.fit(X_train, y_train) \n",
    "prince_mca_X_train = prince_mca.transform(X_train) \n",
    "\n",
    "prince_mca_X_test = prince_mca.transform(X_test) \n",
    "\n",
    "MCA_prince_model.fit(prince_mca_X_train, y_train)\n",
    "\n",
    "pred_train = MCA_prince_model.predict(prince_mca_X_train)\n",
    "pred_test = MCA_prince_model.predict(prince_mca_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sintetica\n",
    "\n",
    "Para el desarrollo de este test, se utilizará la funcion `make_multilabel_classification` de `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos enteros sin OHE (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_multilabel_classification(n_classes=2, n_labels=10,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_synt_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train, y_test)\n",
    "\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.37512632  8.2876105   5.04167236  4.96265839  4.71654274  3.70596353\n",
      "  3.367316    3.22853737  2.77031121  2.44024777]\n",
      "[0.17344275 0.13854539 0.08428249 0.0829616  0.07884724 0.06195322\n",
      " 0.05629199 0.05397201 0.04631176 0.04079404]\n",
      "[0.17344275 0.31198814 0.39627063 0.47923223 0.55807947 0.6200327\n",
      " 0.67632469 0.7302967  0.77660846 0.81740249]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.05408020025947395\n",
      "MSE_Test: 0.043031595426522184\n"
     ]
    }
   ],
   "source": [
    "int_synt_model.fit(PCA_X_train, y_train)\n",
    "pred_train = int_synt_model.predict(PCA_X_train)\n",
    "pred_test = int_synt_model.predict(PCA_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos enteros Con OHE (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_0.0</th>\n",
       "      <th>x0_1.0</th>\n",
       "      <th>x0_2.0</th>\n",
       "      <th>x0_3.0</th>\n",
       "      <th>x0_4.0</th>\n",
       "      <th>x1_0.0</th>\n",
       "      <th>x1_1.0</th>\n",
       "      <th>x1_2.0</th>\n",
       "      <th>x1_3.0</th>\n",
       "      <th>x2_0.0</th>\n",
       "      <th>...</th>\n",
       "      <th>x19_1.0</th>\n",
       "      <th>x19_2.0</th>\n",
       "      <th>x19_3.0</th>\n",
       "      <th>x19_4.0</th>\n",
       "      <th>x19_5.0</th>\n",
       "      <th>x19_6.0</th>\n",
       "      <th>x19_7.0</th>\n",
       "      <th>x19_8.0</th>\n",
       "      <th>x19_9.0</th>\n",
       "      <th>x19_11.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0_0.0  x0_1.0  x0_2.0  x0_3.0  x0_4.0  x1_0.0  x1_1.0  x1_2.0  x1_3.0  \\\n",
       "0      1.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "1      1.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "2      0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "3      0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "4      1.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "95     1.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "96     0.0     1.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "97     0.0     1.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "98     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0   \n",
       "99     0.0     1.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "\n",
       "    x2_0.0  ...  x19_1.0  x19_2.0  x19_3.0  x19_4.0  x19_5.0  x19_6.0  \\\n",
       "0      0.0  ...      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "1      1.0  ...      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "2      0.0  ...      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "3      0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      1.0  ...      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "..     ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "95     0.0  ...      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "96     1.0  ...      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "97     0.0  ...      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "98     0.0  ...      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "99     0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "    x19_7.0  x19_8.0  x19_9.0  x19_11.0  \n",
       "0       0.0      0.0      0.0       0.0  \n",
       "1       0.0      0.0      0.0       0.0  \n",
       "2       0.0      0.0      0.0       0.0  \n",
       "3       0.0      1.0      0.0       0.0  \n",
       "4       0.0      0.0      0.0       0.0  \n",
       "..      ...      ...      ...       ...  \n",
       "95      0.0      0.0      0.0       0.0  \n",
       "96      0.0      0.0      0.0       0.0  \n",
       "97      0.0      0.0      0.0       0.0  \n",
       "98      0.0      0.0      0.0       0.0  \n",
       "99      0.0      1.0      0.0       0.0  \n",
       "\n",
       "[100 rows x 164 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder().fit(X)\n",
    "categorical_bin = ohe.transform(X)\n",
    "categorical_bin = pd.DataFrame(categorical_bin.toarray(), columns=ohe.get_feature_names())\n",
    "categorical_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_synt_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(categorical_bin, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(X_train, y_test)\n",
    "\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81662074 0.73503703 0.71059335 0.64127897 0.62736686 0.58346687\n",
      " 0.53939481 0.51065531 0.47074867 0.45516285 0.43599507 0.43333791\n",
      " 0.41754275 0.40676903 0.39359945 0.36989318 0.3579849  0.33044969\n",
      " 0.30822247 0.29400282 0.28802564 0.27689049 0.26885454 0.26314535\n",
      " 0.25168733 0.23789218 0.22599413 0.21548488 0.20455944 0.20136256\n",
      " 0.18700903 0.18300426 0.167857   0.16119449 0.15534525 0.15196657\n",
      " 0.14634839 0.13635782 0.13345737 0.12636677]\n",
      "[0.05203377 0.04683538 0.04527787 0.04086127 0.03997481 0.03717757\n",
      " 0.03436937 0.03253814 0.02999535 0.02900225 0.02778091 0.0276116\n",
      " 0.02660516 0.02591867 0.02507953 0.023569   0.02281022 0.02105573\n",
      " 0.01963944 0.01873339 0.01835253 0.01764302 0.01713098 0.0167672\n",
      " 0.01603711 0.01515811 0.01439998 0.01373035 0.0130342  0.0128305\n",
      " 0.01191592 0.01166074 0.01069558 0.01027105 0.00989835 0.00968307\n",
      " 0.00932508 0.0086885  0.00850369 0.00805189]\n",
      "[0.05203377 0.09886915 0.14414702 0.18500829 0.2249831  0.26216067\n",
      " 0.29653005 0.32906819 0.35906354 0.38806579 0.4158467  0.44345829\n",
      " 0.47006345 0.49598212 0.52106165 0.54463065 0.56744087 0.5884966\n",
      " 0.60813604 0.62686943 0.64522196 0.66286498 0.67999596 0.69676316\n",
      " 0.71280028 0.72795839 0.74235837 0.75608872 0.76912292 0.78195342\n",
      " 0.79386934 0.80553008 0.81622565 0.82649671 0.83639506 0.84607813\n",
      " 0.85540321 0.86409171 0.8725954  0.88064729]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avocado dataset (Dataset Real - Mixed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin Variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>17074.83</td>\n",
       "      <td>2046.96</td>\n",
       "      <td>1529.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13498.67</td>\n",
       "      <td>13066.82</td>\n",
       "      <td>431.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>13888.04</td>\n",
       "      <td>1191.70</td>\n",
       "      <td>3431.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9264.84</td>\n",
       "      <td>8940.04</td>\n",
       "      <td>324.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>13766.76</td>\n",
       "      <td>1191.92</td>\n",
       "      <td>2452.79</td>\n",
       "      <td>727.94</td>\n",
       "      <td>9394.11</td>\n",
       "      <td>9351.80</td>\n",
       "      <td>42.31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>16205.22</td>\n",
       "      <td>1527.63</td>\n",
       "      <td>2981.04</td>\n",
       "      <td>727.01</td>\n",
       "      <td>10969.54</td>\n",
       "      <td>10919.54</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>17489.58</td>\n",
       "      <td>2894.77</td>\n",
       "      <td>2356.13</td>\n",
       "      <td>224.53</td>\n",
       "      <td>12014.15</td>\n",
       "      <td>11988.14</td>\n",
       "      <td>26.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total Volume     4046       4225    4770  Total Bags  Small Bags  \\\n",
       "0          64236.62  1036.74   54454.85   48.16     8696.87     8603.62   \n",
       "1          54876.98   674.28   44638.81   58.33     9505.56     9408.07   \n",
       "2         118220.22   794.70  109149.67  130.50     8145.35     8042.21   \n",
       "3          78992.15  1132.00   71976.41   72.58     5811.16     5677.40   \n",
       "4          51039.60   941.48   43838.39   75.78     6183.95     5986.26   \n",
       "...             ...      ...        ...     ...         ...         ...   \n",
       "18244      17074.83  2046.96    1529.20    0.00    13498.67    13066.82   \n",
       "18245      13888.04  1191.70    3431.50    0.00     9264.84     8940.04   \n",
       "18246      13766.76  1191.92    2452.79  727.94     9394.11     9351.80   \n",
       "18247      16205.22  1527.63    2981.04  727.01    10969.54    10919.54   \n",
       "18248      17489.58  2894.77    2356.13  224.53    12014.15    11988.14   \n",
       "\n",
       "       Large Bags  XLarge Bags  \n",
       "0           93.25          0.0  \n",
       "1           97.49          0.0  \n",
       "2          103.14          0.0  \n",
       "3          133.76          0.0  \n",
       "4          197.69          0.0  \n",
       "...           ...          ...  \n",
       "18244      431.85          0.0  \n",
       "18245      324.80          0.0  \n",
       "18246       42.31          0.0  \n",
       "18247       50.00          0.0  \n",
       "18248       26.01          0.0  \n",
       "\n",
       "[18249 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avocado_data = pd.read_csv(\"avocado.csv\")\n",
    "categorical_data = avocado_data.select_dtypes(\"O\").drop(\"Date\", axis=1)\n",
    "X_avocado = avocado_data.loc[:, avocado_data.columns != 'Date']\n",
    "X_avocado = X_avocado.loc[:, X_avocado.columns != 'AveragePrice']\n",
    "X_avocado = X_avocado.loc[:, X_avocado.columns != 'year']\n",
    "X_avocado = X_avocado.loc[:, X_avocado.columns != 'type']\n",
    "X_avocado = X_avocado.loc[:, X_avocado.columns != 'region']\n",
    "y_avocado = avocado_data[\"AveragePrice\"]\n",
    "X_avocado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model_avocado = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_avocado, y_avocado, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "pca.fit(X_train, y_test)\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.59132602e+13 1.47215292e+11 1.11923187e+11 1.12251137e+10\n",
      " 2.63431948e+09 1.09330381e+08]\n",
      "[9.83127330e-01 9.09501728e-03 6.91465750e-03 6.93491837e-04\n",
      " 1.62749270e-04 6.75447294e-06]\n",
      "[0.98312733 0.99222235 0.999137   0.9998305  0.99999325 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.1541682995864888\n",
      "MSE_Test: 0.15300610412254645\n"
     ]
    }
   ],
   "source": [
    "PCA_model_avocado.fit(PCA_X_train, y_train)\n",
    "pred_train = PCA_model_avocado.predict(PCA_X_train)\n",
    "pred_test = PCA_model_avocado.predict(PCA_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_conventional</th>\n",
       "      <th>x0_organic</th>\n",
       "      <th>x1_Albany</th>\n",
       "      <th>x1_Atlanta</th>\n",
       "      <th>x1_BaltimoreWashington</th>\n",
       "      <th>x1_Boise</th>\n",
       "      <th>x1_Boston</th>\n",
       "      <th>x1_BuffaloRochester</th>\n",
       "      <th>x1_California</th>\n",
       "      <th>x1_Charlotte</th>\n",
       "      <th>...</th>\n",
       "      <th>x1_SouthCarolina</th>\n",
       "      <th>x1_SouthCentral</th>\n",
       "      <th>x1_Southeast</th>\n",
       "      <th>x1_Spokane</th>\n",
       "      <th>x1_StLouis</th>\n",
       "      <th>x1_Syracuse</th>\n",
       "      <th>x1_Tampa</th>\n",
       "      <th>x1_TotalUS</th>\n",
       "      <th>x1_West</th>\n",
       "      <th>x1_WestTexNewMexico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_conventional  x0_organic  x1_Albany  x1_Atlanta  x1_BaltimoreWashington  \\\n",
       "0              1.0         0.0        1.0         0.0                     0.0   \n",
       "1              1.0         0.0        1.0         0.0                     0.0   \n",
       "2              1.0         0.0        1.0         0.0                     0.0   \n",
       "3              1.0         0.0        1.0         0.0                     0.0   \n",
       "4              1.0         0.0        1.0         0.0                     0.0   \n",
       "\n",
       "   x1_Boise  x1_Boston  x1_BuffaloRochester  x1_California  x1_Charlotte  ...  \\\n",
       "0       0.0        0.0                  0.0            0.0           0.0  ...   \n",
       "1       0.0        0.0                  0.0            0.0           0.0  ...   \n",
       "2       0.0        0.0                  0.0            0.0           0.0  ...   \n",
       "3       0.0        0.0                  0.0            0.0           0.0  ...   \n",
       "4       0.0        0.0                  0.0            0.0           0.0  ...   \n",
       "\n",
       "   x1_SouthCarolina  x1_SouthCentral  x1_Southeast  x1_Spokane  x1_StLouis  \\\n",
       "0               0.0              0.0           0.0         0.0         0.0   \n",
       "1               0.0              0.0           0.0         0.0         0.0   \n",
       "2               0.0              0.0           0.0         0.0         0.0   \n",
       "3               0.0              0.0           0.0         0.0         0.0   \n",
       "4               0.0              0.0           0.0         0.0         0.0   \n",
       "\n",
       "   x1_Syracuse  x1_Tampa  x1_TotalUS  x1_West  x1_WestTexNewMexico  \n",
       "0          0.0       0.0         0.0      0.0                  0.0  \n",
       "1          0.0       0.0         0.0      0.0                  0.0  \n",
       "2          0.0       0.0         0.0      0.0                  0.0  \n",
       "3          0.0       0.0         0.0      0.0                  0.0  \n",
       "4          0.0       0.0         0.0      0.0                  0.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder().fit(categorical_data)\n",
    "categorical_bin = ohe.transform(categorical_data)\n",
    "categorical_bin = pd.DataFrame(categorical_bin.toarray(), columns=ohe.get_feature_names())\n",
    "categorical_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model_avocado_cat = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>x0_conventional</th>\n",
       "      <th>x0_organic</th>\n",
       "      <th>...</th>\n",
       "      <th>x1_SouthCarolina</th>\n",
       "      <th>x1_SouthCentral</th>\n",
       "      <th>x1_Southeast</th>\n",
       "      <th>x1_Spokane</th>\n",
       "      <th>x1_StLouis</th>\n",
       "      <th>x1_Syracuse</th>\n",
       "      <th>x1_Tampa</th>\n",
       "      <th>x1_TotalUS</th>\n",
       "      <th>x1_West</th>\n",
       "      <th>x1_WestTexNewMexico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>17074.83</td>\n",
       "      <td>2046.96</td>\n",
       "      <td>1529.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13498.67</td>\n",
       "      <td>13066.82</td>\n",
       "      <td>431.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>13888.04</td>\n",
       "      <td>1191.70</td>\n",
       "      <td>3431.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9264.84</td>\n",
       "      <td>8940.04</td>\n",
       "      <td>324.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>13766.76</td>\n",
       "      <td>1191.92</td>\n",
       "      <td>2452.79</td>\n",
       "      <td>727.94</td>\n",
       "      <td>9394.11</td>\n",
       "      <td>9351.80</td>\n",
       "      <td>42.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>16205.22</td>\n",
       "      <td>1527.63</td>\n",
       "      <td>2981.04</td>\n",
       "      <td>727.01</td>\n",
       "      <td>10969.54</td>\n",
       "      <td>10919.54</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>17489.58</td>\n",
       "      <td>2894.77</td>\n",
       "      <td>2356.13</td>\n",
       "      <td>224.53</td>\n",
       "      <td>12014.15</td>\n",
       "      <td>11988.14</td>\n",
       "      <td>26.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total Volume     4046       4225    4770  Total Bags  Small Bags  \\\n",
       "0          64236.62  1036.74   54454.85   48.16     8696.87     8603.62   \n",
       "1          54876.98   674.28   44638.81   58.33     9505.56     9408.07   \n",
       "2         118220.22   794.70  109149.67  130.50     8145.35     8042.21   \n",
       "3          78992.15  1132.00   71976.41   72.58     5811.16     5677.40   \n",
       "4          51039.60   941.48   43838.39   75.78     6183.95     5986.26   \n",
       "...             ...      ...        ...     ...         ...         ...   \n",
       "18244      17074.83  2046.96    1529.20    0.00    13498.67    13066.82   \n",
       "18245      13888.04  1191.70    3431.50    0.00     9264.84     8940.04   \n",
       "18246      13766.76  1191.92    2452.79  727.94     9394.11     9351.80   \n",
       "18247      16205.22  1527.63    2981.04  727.01    10969.54    10919.54   \n",
       "18248      17489.58  2894.77    2356.13  224.53    12014.15    11988.14   \n",
       "\n",
       "       Large Bags  XLarge Bags  x0_conventional  x0_organic  ...  \\\n",
       "0           93.25          0.0              1.0         0.0  ...   \n",
       "1           97.49          0.0              1.0         0.0  ...   \n",
       "2          103.14          0.0              1.0         0.0  ...   \n",
       "3          133.76          0.0              1.0         0.0  ...   \n",
       "4          197.69          0.0              1.0         0.0  ...   \n",
       "...           ...          ...              ...         ...  ...   \n",
       "18244      431.85          0.0              0.0         1.0  ...   \n",
       "18245      324.80          0.0              0.0         1.0  ...   \n",
       "18246       42.31          0.0              0.0         1.0  ...   \n",
       "18247       50.00          0.0              0.0         1.0  ...   \n",
       "18248       26.01          0.0              0.0         1.0  ...   \n",
       "\n",
       "       x1_SouthCarolina  x1_SouthCentral  x1_Southeast  x1_Spokane  \\\n",
       "0                   0.0              0.0           0.0         0.0   \n",
       "1                   0.0              0.0           0.0         0.0   \n",
       "2                   0.0              0.0           0.0         0.0   \n",
       "3                   0.0              0.0           0.0         0.0   \n",
       "4                   0.0              0.0           0.0         0.0   \n",
       "...                 ...              ...           ...         ...   \n",
       "18244               0.0              0.0           0.0         0.0   \n",
       "18245               0.0              0.0           0.0         0.0   \n",
       "18246               0.0              0.0           0.0         0.0   \n",
       "18247               0.0              0.0           0.0         0.0   \n",
       "18248               0.0              0.0           0.0         0.0   \n",
       "\n",
       "       x1_StLouis  x1_Syracuse  x1_Tampa  x1_TotalUS  x1_West  \\\n",
       "0             0.0          0.0       0.0         0.0      0.0   \n",
       "1             0.0          0.0       0.0         0.0      0.0   \n",
       "2             0.0          0.0       0.0         0.0      0.0   \n",
       "3             0.0          0.0       0.0         0.0      0.0   \n",
       "4             0.0          0.0       0.0         0.0      0.0   \n",
       "...           ...          ...       ...         ...      ...   \n",
       "18244         0.0          0.0       0.0         0.0      0.0   \n",
       "18245         0.0          0.0       0.0         0.0      0.0   \n",
       "18246         0.0          0.0       0.0         0.0      0.0   \n",
       "18247         0.0          0.0       0.0         0.0      0.0   \n",
       "18248         0.0          0.0       0.0         0.0      0.0   \n",
       "\n",
       "       x1_WestTexNewMexico  \n",
       "0                      0.0  \n",
       "1                      0.0  \n",
       "2                      0.0  \n",
       "3                      0.0  \n",
       "4                      0.0  \n",
       "...                    ...  \n",
       "18244                  1.0  \n",
       "18245                  1.0  \n",
       "18246                  1.0  \n",
       "18247                  1.0  \n",
       "18248                  1.0  \n",
       "\n",
       "[18249 rows x 64 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avocado_ohe = pd.concat([X_avocado,categorical_bin], axis=1)\n",
    "X_avocado_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_avocado_ohe, y_avocado, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "pca.fit(X_train, y_train)\n",
    "## Train\n",
    "PCA_X_train = pca.transform(X_train)\n",
    "\n",
    "## Test\n",
    "PCA_X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.59132602e+13 1.47215292e+11 1.11923187e+11 1.12251137e+10\n",
      " 2.63431948e+09 1.09330381e+08]\n",
      "[9.83127330e-01 9.09501728e-03 6.91465750e-03 6.93491837e-04\n",
      " 1.62749270e-04 6.75447294e-06]\n",
      "[0.98312733 0.99222235 0.999137   0.9998305  0.99999325 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Train: 0.15416829958679562\n",
      "MSE_Test: 0.1530061041226993\n"
     ]
    }
   ],
   "source": [
    "PCA_model_avocado_cat.fit(PCA_X_train, y_train)\n",
    "pred_train = PCA_model_avocado_cat.predict(PCA_X_train)\n",
    "pred_test = PCA_model_avocado_cat.predict(PCA_X_test)\n",
    "\n",
    "print(\"MSE_Train:\",mean_squared_error(pred_train, y_train))\n",
    "print(\"MSE_Test:\",mean_squared_error(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis\n",
    "*  **Abalon Dataset**\n",
    "\n",
    "Se puede ver en los resultados de porcentajes de varianza de la matriz de covarianza para cada una de las componentes de PCA en cada uno de los casos (con y sin variables categóricas) una leve diferencia, menor al 10% en las componentes más significativas. Esta diferencia porcentual menor tiene relación a la forma que se computa PCA, haciendo que sea dificil para el algoritmo entregar un valor o un peso para poder relacionarlo con los otros atributos del dataset, entregando resultados que no tienen mucha información y terminan por añadir solamente ruido al dataset. Adicionalmente, esta diferencia puede ser relacionada con la inclusión de nuevas componentes al agregar más dimensiones con el OHE (One-Hot Encoding), que entrega tres columnas para una columna categorica (columna `Sex` con valores `M, F, I`).\n",
    "\n",
    "También, se puede ver que no existen diferencias significativas en los errores cuadráticos medios de los modelos aplicados a los datos con PCA y sin PCA, que están relacionados con la metodología de cómputo de PCA y la introducción de ruidos al agregar columnas adicionales.\n",
    "\n",
    "Adicionalmente, se puede ver que el método MCA, que funciona mejor para datos categoricos, tampoco nos entrega resultados mucho mejores, debido a la naturaleza de nuestros datos, que correponde a una mixta (datos continuos y datos categoricos), lo que nos hace pensar que un método que aplique PCA y MCA sobre nuestros atributos pueda entregar resultados más significativos\n",
    "\n",
    "* **Avocado Dataset**\n",
    "\n",
    "Del mismo modo que para el Abalon dataset, las diferencias que se tuvieron fueron no significativas, lo que nos confirma lo propuesto anteriormente, en especial a la diferencia que existe debido a la inclusión de nuevas columnas al dataset, y por lo tanto, nuevas componentes a PCA.\n",
    "\n",
    "* **Dataset Sintéticos**\n",
    "\n",
    "Para los datos sínteticos se trabaja con datos de naturaleza únicamente categorica, para analizar mejor el comportamiento de PCA dentro de estas condiciones. En este dataset las categorias estan representadas por números enteros en primera instancia y que posteriormente se codifican utilizando OHE. En el primer caso, si bien los datos tienen un peso asociado a su valor, este valor no entrega un significado al modelo, por lo que nuevamente el cálculo de PCA se verá afectado por estas condiciones y los resultados que entrega no muestran una correlación entre los distintos atributos que existen dentro de los datos. Esta condición se ve mucho más claramente al hacer el OHE, debido a que nuestros atributos de ser cerca de 20 a ser 160 aproximadamente, y por lo tanto nuestra cantidad de componentes también aumenta drásticamente. Por esto último se modifican el porcentaje de componentes que se le solicitarán a PCA respecto a la cantidad de atributos, pasando de ser un 50% para el caso sin OHE a 25% con OHE. Este cambio confirma nuevamente la introducción de ruido dentro del dataset, donde una disminución porcentual significativa dentro de las componentes de PCA, entrega un porcentaje de varianza acumulado similar a la que se obtenía sin introducir OHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío Kaggle (Mercado Libre) - Desarrollo\n",
    "*Objetivo: El problema propuesto consiste en predecir la categoría a la que pertenece un anunció a partir de su título*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /users/cnunez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/cnunez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    os.system('pip install -U imbalanced-learn')\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    os.system('pip install nltk')\n",
    "    import nltk\n",
    "#nltk.download('all-corpora')\n",
    "#nltk.download('all')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2uEPJgrSuXX"
   },
   "source": [
    "Comenzamos cargando el dataset de la competencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7zMImYTgSuXZ"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion Y\n",
    "A continuacion, definimos la variable `Y` como el conjunto de etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_train.category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQCjoDYtSuYs"
   },
   "source": [
    "## Pre-procesamiento de texto\n",
    "\n",
    "Dado que la columna títulos corresponde a un texto, debemos procesar el texto por medio de una tokenización, lematización, y Stemming. Para ello, utilizamod la librería `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import re\n",
    "#nlp_es = spacy.load('es_core_news_sm')\n",
    "\n",
    "import spacy\n",
    "#nlp = spacy.load(\"es_core_news_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "stops_es = stopwords.words(\"spanish\")\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemNormalizeSpacy(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    text_lower = text.lower().translate(remove_punct_dict)\n",
    "    doc = nlp(text_lower)\n",
    "    solo_lemmas=[]\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            lemma = token.lemma_.strip()  # get the lemma\n",
    "            lemma = unidecode.unidecode(lemma)  # Remove accents\n",
    "            lemma = re.sub(r'\\d+', '',lemma)  # Remove numbers\n",
    "            lemma = re.sub(r'_', '', lemma)\n",
    "            if lemma and lemma not in stops_es:\n",
    "                solo_lemmas.append(lemma)\n",
    "    return solo_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "r2vO2wqqSuZD"
   },
   "outputs": [],
   "source": [
    "tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "remove_stops_here = True\n",
    "\n",
    "def my_pre_processer(text):\n",
    "    results = []\n",
    "    for token in tokenizer(text):\n",
    "        clean_token = token.lower().strip().strip('-').strip('_')\n",
    "        if remove_stops_here and (clean_token in stopwords.words('spanish')):\n",
    "          continue\n",
    "        #token_pro = stemmer.stem(clean_token) #podemos probar stemming en vez de lematizacion\n",
    "        token_pro = lemmatizer.lemmatize(clean_token) \n",
    "        if len(token_pro) > 2 and not token_pro[0].isdigit(): #elimina palabra largo menor a 2\n",
    "            results.append(token_pro)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de como sería el pre-procesamiento es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgOPMahzSuZG",
    "outputId": "45d90e53-4db7-4936-b8ba-97fc54cdb48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kit 4 Bujias Ngk Ford F-100 3.0 Bp5efs\n",
      "['kit', 'bujias', 'ngk', 'ford', 'bp5efs']\n"
     ]
    }
   ],
   "source": [
    "print(df_train.title.values[1])\n",
    "print(my_pre_processer(df_train.title.values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kit', 'bujias', 'ngk', 'ford', 'f', 'bpefs']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemNormalizeSpacy(df_train.title.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora toca una de las partes más importantes, que es vectorizar la representación del texto para cada título del dataset. Para ello utilizaremos la clásica técnica `Tf-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FIkrzlyUSuZS"
   },
   "outputs": [],
   "source": [
    "max_df=0.9\n",
    "min_df=5\n",
    "max_features=100000\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemNormalizeSpacy, min_df=min_df, max_df=max_df, \n",
    "                             max_features=max_features, use_idf=True, smooth_idf=True, \n",
    "                             strip_accents='unicode', analyzer = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjxpoClPSuZW",
    "outputId": "3953f49e-af5d-4da3-c566-1667c39c828b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación demoró 1509.8772060871124[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "vectorizer.fit(df_train.title)\n",
    "X = vectorizer.transform(df_train.title)\n",
    "end_time = time()\n",
    "\n",
    "print('La operación demoró {}[s]'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHBtql2cSuZc"
   },
   "source": [
    "## Balanceamiento del dataset - SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDNgcGSOSuZd"
   },
   "source": [
    "Dado que el dataset puede estar mal condicionado o desbalanceado, utilizaremos la técnica de `SMOTE` para generar un sample balanceado de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5S2goXuRSuZj"
   },
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state = 42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkazWLqbSuZl",
    "outputId": "51b9b8e0-0f96-420a-a91c-3b4305f43a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación demoró 6.832873106002808[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "X,Y = oversample.fit_resample(X, Y)\n",
    "end_time = time()\n",
    "\n",
    "print('La operación demoró {}[s]'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DDnaTFsSuZz"
   },
   "source": [
    "## Diseño del clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb99ehkaSuZ0"
   },
   "source": [
    "Para cumplir con nuestro objetivo, utilizaremos un regreso logistico entrenaddo con la tecnica OVR para clasificar las distintas categoricas de los productos. Por otro lado, se utilizará la implementación `LogisticRegressionCV` para incluir cross-validation en el entrenamiento.\n",
    "\n",
    "**Declaimer**: Esta parte puede demorar muchos más en comparación al resto del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "m8N-62y8SuZ3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.3s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.9s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.2s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.1s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   10.5s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.8s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.4s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.1s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.8s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.1s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.7s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.9s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.6s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.9s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    9.1s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.9s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.5s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.3s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.1s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.0s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.9s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.4s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.4s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.9s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.2s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.7s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    7.7s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.8s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación demoró 409.74054312705994[s]\n"
     ]
    }
   ],
   "source": [
    "base = LogisticRegressionCV(cv=10, random_state=42, max_iter=10000, solver='newton-cg', n_jobs=-1, verbose=1)\n",
    "clf = OneVsRestClassifier(base)\n",
    "\n",
    "start_time = time()\n",
    "clf.fit(X,Y)\n",
    "end_time = time()\n",
    "\n",
    "print('La operación demoró {}[s]'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnbLo_BFSuZ-"
   },
   "source": [
    "## Predicción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos vectorizando los títulos que debemos predecir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofJXP5LRSuZ-",
    "outputId": "9752d390-174d-47ae-b3a5-7a3e661dc0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación demoró 510.38388991355896[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "vector_titles_submission = vectorizer.transform(df_test.title)\n",
    "end_time = time()\n",
    "\n",
    "print('La operación demoró {}[s]'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego realizamos la predicción sobre el set de datos de la competencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "dlpy2l9-SuaF"
   },
   "outputs": [],
   "source": [
    "y_pred_submission = clf.predict(vector_titles_submission)\n",
    "df_test.Predicted = y_pred_submission\n",
    "\n",
    "df_test = df_test.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente guardamos el dataset final con las etiquetas para realizar el submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d541nT2bSuaL",
    "outputId": "e0dea09b-4c2a-4811-9100-c86fcd867465"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('last_submission.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
